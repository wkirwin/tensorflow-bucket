{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Chapter 14 of _Hands on Machine Learning with Scikit-Learn and Tensorflow_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bare 2-step RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](simpleRNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to unroll it 2 steps and set the hidden vector equal to the output vector (a _very_ simple RNN). That is:\n",
    "\n",
    "$$ Y_0 = \\tanh(W_x X_0 + b) $$\n",
    "\n",
    "and  \n",
    "\n",
    "$$ Y_1 = \\tanh(W_x X_1 + W_y Y_0 + b) $$\n",
    "\n",
    "We'll pass batches of inputs in: a numpy array of size `[n_batches, n_inputs]`. That is, we're writing $X_t$ as a row vector, so really $Y_0 = \\tanh(X_0^t W_x^t + b^t),$ but we'll just drop all the transposes. We should have:\n",
    "\n",
    "* $X_0, X_1$: `[n_batches, n_inputs]`\n",
    "* $W_x$: `[n_inputs, n_neurons]`\n",
    "* $W_y$: `[n_neurons, n_neurons]`\n",
    "* $b$: `[1, n_neurons] `\n",
    "\n",
    "($b$ will be broadcast to `[n_batches, n_neurons]`). That means the output will be\n",
    "\n",
    "* $Y_t$: `[n_batches, n_neurons]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "n_neurons = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons], dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, the following (first) output of the RNN should be a numpy array of size `[n_batches, n_neurons] = [4, 5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "[[ 0.8960636  -0.9961354   0.9960467  -0.9680858   0.44344255]\n",
      " [ 0.4309014  -1.          0.9999999  -0.9999996  -0.8754126 ]\n",
      " [-0.4852839  -1.          1.         -1.         -0.99660254]\n",
      " [-0.97851205 -0.999997   -0.969897   -0.34225827 -0.9942562 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y0_val.shape)\n",
    "print(Y0_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for the following (second) output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "[[-0.9999995  -1.          1.         -1.         -0.99999994]\n",
      " [-0.69430953  0.9957111  -0.9075036  -0.4842649  -0.95532596]\n",
      " [-0.99352264 -0.9999998   0.99997026 -1.         -0.99999714]\n",
      " [-0.9733558  -0.9990728   0.97920734 -0.9783926  -0.9841459 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y1_val.shape)\n",
    "print(Y1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static 2-step Unrolling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cell = tf.keras.layers.SimpleRNNCell(units=n_neurons)  # replaces the deprecated tf.contrib.rnn.BasicRNNCell\n",
    "output_seq, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0, Y1 = output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()  # this needs to be defined here... defining it before the variables does bad things.\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "[[ 0.26076204 -0.87741923  0.9683349   0.87098503 -0.67625916]\n",
      " [ 0.9233953  -0.9907382   0.999999    0.9835779  -0.99912375]\n",
      " [ 0.99460393 -0.9993372   1.          0.998014   -0.99999803]\n",
      " [ 0.9606342   0.34861976  0.9994729  -0.9944234  -0.9970318 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y0_val.shape)\n",
    "print(Y0_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "[[ 0.99945015 -0.9993708   1.          0.984867   -0.9999999 ]\n",
      " [ 0.13449268 -0.9358899   0.5924583   0.74528295 -0.60351944]\n",
      " [ 0.98775476 -0.9950965   0.99999994  0.941673   -0.9999885 ]\n",
      " [ 0.48508894 -0.589504    0.94871217  0.7335965  -0.9837607 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y1_val.shape)\n",
    "print(Y1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static n-step Unrolling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite the above in a more scalable way. The input will be a placeholder of shape `[batch_size, n_steps, n_inputs]`, which in TensorFlow can be writte `[None, n_steps, n_inputs]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second argument to `tf.contrib.rnn.static_rnn` should be the list `[X0, X1, ...]`. Each `X_j` is, as above, a tensor of shape `[batch_size, n_inputs]`, that is, a tensor of shape `[None, n_inputs]`. So we have to massage `X` into such a list. \n",
    "\n",
    "We can use `tf.unstack` for this; it turns a tensor of shape `(A, B, C)` into a list of tensors of shape `(B, C)`, where the `i`-th component of the list is the tensor at `A = i`. We want to unstack along `B` (so that we get shape `[None, n_inputs]`), so we'll also pass `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seqs = tf.unstack(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cell = tf.keras.layers.SimpleRNNCell(units=n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, X_seqs, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that each of the `n_steps` outputs has the expected shape `[None, n_neurons]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(5)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seqs[0].get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we restack the outputs into a single tensor so that our output has shape `[None, n_steps, n_neurons]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.stack(output_seqs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2), Dimension(5)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out. Here's a batch of input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = np.array([\n",
    "    [[0, 1, 2], [9, 8, 7]],  # instance 0 time steps 1 and 2\n",
    "    [[3, 4, 5], [0, 0, 0]],  # instance 1\n",
    "    [[6, 7, 8], [6, 5, 4]],  # instance 2\n",
    "    [[9, 0, 1], [3, 2, 1]],  # instance 3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.9499117   0.14086019  0.6971834  -0.2441735   0.45512706]\n",
      "  [-0.9373555  -0.99562085  0.9999227   0.99250203  0.99996305]]\n",
      "\n",
      " [[-0.9902277  -0.74542236  0.9922384   0.64982086  0.9879935 ]\n",
      "  [-0.8743683   0.7260192  -0.69016814 -0.01985344 -0.6202365 ]]\n",
      "\n",
      " [[-0.9981246  -0.9684646   0.9998299   0.9467215   0.9998052 ]\n",
      "  [-0.8056482  -0.86899126  0.9493427   0.9825388   0.9915183 ]]\n",
      "\n",
      " [[ 0.99998075 -0.9627134   0.98568964  0.03348969  0.99999964]\n",
      "  [-0.05042231 -0.9036694   0.18797247  0.9902675   0.94957983]]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Unrolling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following does the same thing as the above code (although `dynamic_rnn` does some magic during backprop and can swap gpu  to cpu memory to avoid OOM for large `n_steps` if you set `swap_memory=True`). \n",
    "\n",
    "It handles the stacking and unstacking internally, and takes an input of shape `[None, n_steps, n_inputs]` and outputs a shape `[None, n_steps, n_neurons]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "basic_cell = tf.keras.layers.SimpleRNNCell(units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = np.array([\n",
    "    [[0, 1, 2], [9, 8, 7]],  # instance 0 time steps 1 and 2\n",
    "    [[3, 4, 5], [0, 0, 0]],  # instance 1\n",
    "    [[6, 7, 8], [6, 5, 4]],  # instance 2\n",
    "    [[9, 0, 1], [3, 2, 1]],  # instance 3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify: outputs_val should have shape `[4, 2, 5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.7926984   0.7797086  -0.6041814   0.9648911   0.27033582]\n",
      "  [ 0.6572988  -0.56955177  0.9949784   1.         -0.63364404]]\n",
      "\n",
      " [[ 0.92245036  0.86741316 -0.02894654  0.99999976  0.31256512]\n",
      "  [ 0.25936666 -0.5994076  -0.00476568  0.89737266 -0.15621978]]\n",
      "\n",
      " [[ 0.9722467   0.92173547  0.56612206  1.          0.35359406]\n",
      "  [ 0.215779   -0.8811101   0.9424212   1.          0.15000457]]\n",
      "\n",
      " [[-0.5318223  -0.50594455  0.9999975   0.999985    0.93945324]\n",
      "  [ 0.71124107 -0.6304598   0.83129096  0.99943846  0.72069645]]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable-length Sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to pass a list of sequence lengths to `dynamic_rnn` if we want to handle sequences of variable length. \n",
    "\n",
    "The input should still be of shape `[None, n_steps, n_inputs]`, but we set `n_inputs = max(seq_length)` and zero pad shorter sequences. So if we want to pass the four sequences\n",
    "```\n",
    "[0, 1, 2], [9, 8, 7], [6, 5, 4]  # seq of length 3\n",
    "[1, 1, 0]  # seq of length 1\n",
    "[1, 2, 3], [5, 8, 7]  # seq of length 2\n",
    "[1, 2, 2], [3, 3, 1], [1, 2, 3]  # seq of length 3\n",
    "```\n",
    "we set our batch to be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = np.array([\n",
    "    [[0, 1, 2], [9, 8, 7], [6, 5, 4]],  # seq 0\n",
    "    [[1, 1, 0], [0, 0, 0], [0, 0, 0]],  # seq 1\n",
    "    [[1, 2, 3], [5, 8, 7], [0, 0, 0]],  # seq 2\n",
    "    [[1, 2, 2], [3, 3, 1], [1, 2, 3]]\n",
    "])\n",
    "seq_length_batch = np.array([3, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "n_steps = 3\n",
    "n_neurons = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.keras.layers.SimpleRNNCell(units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32, sequence_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run([outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the outputs, we should see the second output (`Y_1`) a length-1 sequence of length-5 vectors with zero padding (i.e., 2 length-5 zero vectors). The third output (`Y_2`) should be a length-2 sequence with 1 zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.65343726  0.6189102   0.97039735 -0.1816915   0.6944628 ]\n",
      "  [ 0.99998784  0.89341444  0.98185945 -1.          0.9999708 ]\n",
      "  [ 0.9992925  -0.52434665  0.587328   -0.999998    0.98419636]]\n",
      "\n",
      " [[ 0.9017867  -0.4601119  -0.02660421 -0.8641541   0.38876387]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.09805908  0.7601289   0.99115115 -0.8503448   0.9087573 ]\n",
      "  [ 0.99613225 -0.45343968  0.9999848  -0.99999875  0.999915  ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.6046406   0.22202344  0.9688044  -0.90392137  0.8529464 ]\n",
      "  [ 0.9960751  -0.97410154 -0.07563845 -0.9993654   0.8751569 ]\n",
      "  [ 0.534737   -0.45697638  0.99398696 -0.97692496  0.9358953 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important point: in this `SimpleRNNCell`, the hidden state is just the last output vector. So if we want the final states for each entry in the batch, we can just look at `states_val`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9992925  -0.52434665  0.587328   -0.999998    0.98419636]\n",
      " [ 0.9017867  -0.4601119  -0.02660421 -0.8641541   0.38876387]\n",
      " [ 0.99613225 -0.45343968  0.9999848  -0.99999875  0.999915  ]\n",
      " [ 0.534737   -0.45697638  0.99398696 -0.97692496  0.9358953 ]]\n"
     ]
    }
   ],
   "source": [
    "print(states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it handled the zero-padding correctly. It picked of the _final_ states given the `seq_length`s, not just the highest-index state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
